{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Homework, Week 1, part 3\n",
    "\n",
    "*Part of the course:\n",
    "Introduction to Machine Learning (code: KI2V20001), 26/04/2021 to 02/07/2021, Utrecht University*\n",
    "\n",
    "Total points: 35\n",
    "\n",
    "Submit one ipynb file per pair, with filename: ```IML2021_week1_part4_wg#_lastname1_lastname2.ipynb```, with your group number or name in place of the # (if you're in different groups, pick one)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment 1** (8 points)\n",
    "\n",
    "\"Consider a bin that contains red and green marbles, possibly infinitely many. The proportion of red and green marbles in the bin is such that if we pick a marble at random, the probability that it will be red is $\\mu$ and the probability that it will be green is $1-\\mu$. We assume that the value of $\\mu$ is unknown to us.\n",
    "\n",
    "We pick a random sample of N independent marbles (with replacement) from this bin, and observe the fraction v of red marbles within the sample.\" (From: Learning From Data (Abu-Mostafa et al, 2012))\n",
    "\n",
    "\n",
    "1. (3) If $\\mu = 0.7$, use the Hoeffding Inequality to bound the probability that a sample of 8 marbles will have $\\nu \\leq 0.1$. (Based on Exercise 1.9 from Learning From Data (Abu-Mostafa et al, 2012).)\n",
    "* **Answer** Hoeffding's inequality, as formulated in the book of Abu-Mostafa et al (2012)  states<br>\n",
    "   $P(|\\nu - \\mu| \\gt \\epsilon) \\leq 2e^{-2n\\epsilon^2}$ with $n=8$.<br>\n",
    "   We know the true mean $\\mu$ which is 0.7. If $\\nu$ is to be less than 0.1, this means the absolute distance to $\\mu$ must be at least 0.6. We can thus plug in $n=8,\\epsilon=0.6$ on the RHS and obtain $2e^{-2n\\epsilon^2}=2e^{-2\\cdot8\\cdot0.6^2}\\approx 0.006$, see also numpy below.<br>\n",
    "   <span style=\"color:green\">Note: the observant student may notice that what we need is \"$|\\nu - \\mu| \\geq \\epsilon$\" in the Hoeffding equation, however, the book uses \"$\\gt$\". Fortunately, this hardly matters (why, do you think?).</span>\n",
    "2. (3) If $\\mu = 0.8$, find the smallest value for $N$ that one can find with the Hoeffding inequality for which holds that $\\nu$ deviates *at most* $0.05$ from $\\mu$ with a probability of at least $0.99$. (Note that the problem has been formulated in a reversed way from the book. In the book the probability represents the *undesired* situation that $\\mu$ and $\\nu$ deviate more than a predefined threshold. Here it is about the *desired* situation that they deviate at most a given threshold.).\n",
    "* **Answer** Using a basic property of probabilities, we can write:<br>\n",
    "(1) $P(|\\nu - \\mu| \\gt \\epsilon) = 1 - P(|\\nu - \\mu| \\lt \\epsilon)$<br>\n",
    "Moreover, using Hoeffding, we now also know that:<br>\n",
    "(2) $\\lgroup 1 - P(|\\nu - \\mu| \\lt \\epsilon) \\rgroup \\leq 2e^{-2n\\epsilon^2}$<br>\n",
    "By some simple inequality rewriting, we see that:<br>\n",
    "(3) $P(|\\nu - \\mu| \\lt \\epsilon) \\gt 1 - 2e^{-2n\\epsilon^2}$<br>\n",
    "(It is obvious that the sign has to flip, after all, we have multiplied both sides of the inequality with -1.)<br>\n",
    "Given the requirements in the question, we desire:<br>\n",
    "(4) $P(|\\nu - \\mu| \\lt 0.05) \\gt 0.99$<br>\n",
    "From this we see that $\\epsilon = 0.05$. And we see that if the righthand side of (3) is at least 0.99, we know we are safe. After all, it is a *lower* bound. It is exactly 0.99 when:<br>\n",
    "(5)  $1 - 2e^{-2n\\epsilon^2} = 0.99$<br>\n",
    "Filling in the $\\epsilon = 0.05$ just obtained, and solving for $n$, we get:<br>\n",
    "(6) $n = \\lceil\\frac{\\ln\\lgroup\\frac{-(0.99 - 1)}{2}\\rgroup}{-2\\cdot0.05^2}\\rceil = 1060 $<br>\n",
    "($\\lceil x \\rceil$ means round to the nearest higher integer.) See verification of this value below, using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extra, not required. Calculating the value using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060.0\n"
     ]
    }
   ],
   "source": [
    "from numpy import log, ceil\n",
    "\n",
    "n = ceil(log((1-0.99)/2)/(-2*0.05**2))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. (2) Akwasi calculates the previous question in another way, and finds a value that is smaller than you found. Is it certain that he made a mistake (assuming you made no mistake in applying Hoeffding)? Why? \n",
    "* **Answer** No, that is not certain. Akwasi may have done everything correctly, and may have found a better value for $n$. After all, Hoeffding's inequality only gives an *upper bound* (or with the reverse formulation in the last question, a *lower bound*). Someone may find a sharper bound with another method than Hoeffding. In other words, that is why Hoeffding's inequality is called an *inequality* and not an *equality*. The true value for the given probability can be anywhere within the bound given by Hoeffding's.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment 2** (8 points)\n",
    "\n",
    "Suppose that you are confronted with a machine learning problem that is completely unknown to you. The data-set is offered by an alien from a completely different universe, and no context whatsoever is known. Assume that the target function is binary, and has an infinite input space, and could truly be any binary target function. (This is a stronger version of Exercise 1.12 from Learning From Data (Abu-Mostafa et al, 2012).)\n",
    "\n",
    "Tip: first thoroughly read Section 1.3.3 in the book. In particular, focus on \"The feasibility of learning is thus split into two questions\" up to the end of the section. This text explain an essential part of the intuition behind (statistical) machine learning.\n",
    "\n",
    "1. (2) Answer Exercise 1.12 from the book.\n",
    "* **Answer** c. In the rest of the answers, an explanation will follow.\n",
    "2. Now, return to the alien's dataset. Suppose that you decide to put *any* possible function in your hypothesis set $\\mathcal{H}$ . Also, assume that you have some way to find the hypothesis $h$ with the lowest $E_\\textrm{in}$.\n",
    "    1. (1) What can you say about the value of $E_\\textrm{in}$?\n",
    "    * **Answer** It has been given that we have a learning algorithm that can always find the hypothesis with the lowest $E_\\textrm{in}$. Because every possible function is in our hypothesis set, there is at always at least one perfect match for our data in the hypotheses set. (In fact, there are infinitely many.). Moreover, it has been given that we have a learning algorithm that can always find the hypothesis with the lowest $E_\\textrm{in}$. So $E_{in}$ is always 0. Good news.\n",
    "    2. (1) What can you say about how close $E_\\textrm{in}$ and $E_\\textrm{out}$ are together? So, what is the value of $P(|E_\\textrm{in} - E_\\textrm{out}| > \\epsilon)$. Consider any possible value for $\\epsilon$.\n",
    "    * **Answer** Here the situation is opposite. This easy to see as follows. Outside of the dataset, the chosen hypothesis can literally be any function. So, in other points than the dataset, it can be defined in any possible way. After all, the hypothesis set contains any possible function. Moreover, the target function can also be any possible function. This means that the probability that you would have chosen an hypothesis that has some correlation with the target function is 0. (Note, the hypothesis will not perform better than chance, so $E_\\textrm{out} = 0.5$, which is the worst possible value.)  So, $E_\\textrm{in}$ is very far removed from $E_\\textrm{out}$\n",
    "    * Extra reflection: the conclusion is that, althought $E_{in} = 0$ your hypothesis is completely useles because $E_\\textrm{in}$ is not in any way representative for $E_\\textrm{out}$, i.e. the out of sample, so the **true**, performance of the hypothesis. This is somewhat analogous to any other measurement-and-control situation, for example,  with a thermometer and cooling/heating elements. Suppose, the ideal temperature for you is 25$^{\\circ}$C degrees. You book a hotel room and the owner tells you that you can remotely adjust the temperature before you arrive at the room in the evening. You can do so by remotely playing with a cooling and a heating element in the room, while remotely reading a thermomether, all installed in the room. The cooler and the heater are so strong, that you can reach any possible temperature. And indeed, you succeed. First you are very happy. However, then you find out, from the manufacturer, that the thermometer is broken and has a gigantic measuring error. Then you'll know the reading is completely useless, and also you do not know whether you have reached the right temperature.\n",
    "3. Now, suppose that you decide to choose a less expressive hypothesis set. For example, you limit $\\mathcal{H}$ to all functions that a perceptron can learn.\n",
    "    1. (1) What can you say about the value of $E_\\textrm{in}$?\n",
    "    * **Answer** Because the target function can be everything, the probability that the limited hypothesis set even contains something that is close to the target function is 0. There, we know that the performance of the hypothesis chosen will be (close to) $E_{in} = 0.5$, the worst possible error, because it is similar to chance (i.e., tossing a coin instead of using the hypothesis, would not be worse.)\n",
    "    2. (1) What can you say about the \"generalisation of the error\" from in-of-sample to out-of-sample? So, what is the value of $P(|E_\\textrm{in} - E_\\textrm{out}| > \\epsilon)$. Consider any possible value for $\\epsilon$.\n",
    "    * **Answer** Here the situation is opposite. $E_{out}$ will also be $0.5$ (by a similar reasoning as in the previous question). So, $E_\\textrm{in}$ is very close to $E_\\textrm{out}$. In terms of $\\epsilon$. TODO finish.\n",
    "    * Extra reflection: the conclusion is, however, the same. The hypothesis is completely useles. Although $E_\\textrm{in}$ is very close to $E_\\textrm{out}$ (a good thing), we also know that $E_\\textrm{in} = 0.5$ (which is the worst possible outcome). In terms of the room temperature measurement-control system:  the manufacturer just told you that the thermometer has been repaired and gives perfect readings. You are now very happy. Unfortunately, it turns out that the heater and cooler have been replaced by something very inferior. Whatever you try with them, you can not get anywhere near the 25$^{\\circ}$C degrees. The temperature-control instruments simply do not have sufficient capacity to reach your desired goal. (Similar to: the hypothesis set is too small to reach anywhere near the target function.) The case with the Alien's dataset is infinitely worse than this case, it would comparable to having a desired temperature of 25$^{\\circ}$C degrees, while you cannot get it above -273$^{\\circ}$C (absolute zero), whatever you try with the heater.\n",
    "1. (2) This solution is not provided in the multiple-choice question formed by Exercise 1.12. So, what does Exercise 1.12 tacitly assume?\n",
    "\n",
    "* **Answer** The tacit assumption is that the target function cannot be *anything* but that something is known about it. It must belong to a more restricted class of functions. You can imagine that this is the case, because you and the woman are from the same world, Earth, and we are member of the same species, sharing a great part of our history and biology. So, probably functions that make sense to us are from a more limited class then all possible functions. So, you *do* know something about the dataset, and may be able to make an educated guess about a viable hypothesis set. But what you know is very little, so the probability that you will learn something is very low, albeit not zero. That is why the answer to the question in the book is (c). In the case of the alien, there is truly nothing you know, per definition of the question formulation. Therefore you do not stand any chance of learning anything from the data. For that, there should be added one solution to the question in the book: (d) you'll fail anyway. Poor you. Or better, poor alien.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "**Assignment 3** (19 points, and 3 bonus-points)\n",
    "\n",
    "\"Consider the perceptron in two dimensions: $h(\\mathbf{x}) = sign(\\mathbf{w}^{T}\\mathbf{x})$ where $\\mathbf{w} = [w_0 , w_1 , w_2]^T$ and $\\mathbf{x} = [1, x_1 , x_2]^T$. Technically, $\\mathbf{x}$ has\n",
    "three coordinates, but we call this perceptron two-dimensional because the first\n",
    "coordinate is fixed at 1.\" (From: Problem 1.2 of Learning From Data (Abu-Mostafa et al, 2012).)\n",
    "\n",
    "1. (3) The decision boundary is formed by the region in the input space between the regions where the perceptron classifies the input as $+1$ and $-1$. Show that this decision boundary is a line. Tip: what is the value that $h(\\mathbf{x})$ has exactly on this decision boundary, when you do not apply the \"sign\" function? Using that equation, express $x_2$  in terms of the other variables. Be aware that in this context $x_2$ plays the role of $y$ and $x_1$ that of $x$ as you know from good old secondary school mathematics.\n",
    "1. (2) (With pen and paper/a graphical tool.) Draw a graph of the decision boundary for the case $\\mathbf{w} = [1, 1, 1]^T$\n",
    "\n",
    "For the following questions, assume that there is no bias, so $w_0 = 0$. So, from now on $\\mathbf{w} = [0, 1, 1]^T$.\n",
    "\n",
    "3. (2) (With pen and paper/a graphical tool.) Draw a graph of the decision boundary for the case $\\mathbf{w} = [0, 1, 1]^T$.\n",
    "1. (2) Draw $\\mathbf{w}$ in the same graph, but limit yourself to $w_1$ and $w_2$ only. So, draw the weight-vector $[w_1, w_2]^T$.\n",
    "1. (2) In the same graph, indicate the regions that are $+1$ and $-1$ by drawing + and - symbols at the right sides of the decision boundary.\n",
    "1. (2) Multiply $\\mathbf{w}$ by $-1$.  (So, create $\\mathbf{w}' = -\\mathbf{w} = [-w_0 (=0) , -w_1 , -w_2]^T$.) Create a new graph, and repeat the things you did in the previous three questions.\n",
    "1. (2) Multiply $\\mathbf{w}$ with a constant for which holds both $c > 0$ and $c \\neq 1$, creating vector $\\mathbf{w}''$ and do the same. So, create a graph and draw the decision boundary, weight-vector and +1 and -1 regions in it.\n",
    "1. (2) Now, change the signs of the weight-vector elements of $\\mathbf{w}$ such that they are opposing. So, consider $\\mathbf{w}''' = [0 , -1 , 1]^T$ and $\\mathbf{w}'''' = [0 , 1 , -1]^T$. Draw two graphs for each of these cases as well, with the decision boundary, weight-vector and +1 and -1 regions.\n",
    "1. (2) What could be the relation of the direction of the weight-vector and the direction of the decision boundary, you think? And what is the relation between the direction of the weight vector and the regions +1 and -1?\n",
    "1. (bonuspoints 3, not required$^*$) prove the relation discovered in the previous question, also for the cases $\\mathbf{w}$'s elements can take *any* value.\n",
    "\n",
    "$^*$ bonus points count in the following way: your points for this part will be $\\textrm{min}(35, \\textrm{[total points including bonuspoints]})$. So, bonus points are added to your total, but not beyond the maximum for this part.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
